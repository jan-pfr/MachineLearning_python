{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0)\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "# a 2D array that represent every possible state and action in the virtual space and initialize all of them to 0\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "exploration = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample() # Explore a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "            \n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        \n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40090669, -2.41412198, -2.41767969, -2.3639511 , -6.84836069,\n",
       "       -8.62169302])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.12208981, -2.23981204, -2.25062334, -2.22939021, -7.50948405,\n",
       "       -7.91650559])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[streets.encode(1,0,2,0)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 13\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "13.9\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "lengths=[]\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()\n",
    "   \n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        print(streets.render(mode='ansi'))\n",
    "        sleep(.2)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "    lengths.append(trip_length)\n",
    "    \n",
    "    sleep(.2)\n",
    "avg_len=sum(lengths)/10\n",
    "print(avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(learning_rate,discount_factor,exploration,epochs):\n",
    "    q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "# a 2D array that represent every possible state and action in the virtual space and initialize all of them to 0\n",
    "    for taxi_run in range(epochs):\n",
    "        state = streets.reset()\n",
    "        done = False\n",
    "            \n",
    "        while not done:\n",
    "            random_value = random.uniform(0, 1)\n",
    "            if (random_value < exploration):\n",
    "                action = streets.action_space.sample() # Explore a random action\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "                    \n",
    "            next_state, reward, done, info = streets.step(action)\n",
    "                \n",
    "            prev_q = q_table[state, action]\n",
    "            next_max_q = np.max(q_table[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table[state, action] = new_q\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "\n",
    "\n",
    "def average_trip_length():\n",
    "    lengths=[]\n",
    "    for tripnum in range(1, 11):\n",
    "        state = streets.reset()\n",
    "        done = False\n",
    "        trip_length = 0\n",
    "        \n",
    "        while not done and trip_length < 25:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, done, info = streets.step(action)\n",
    "            clear_output(wait=True)\n",
    "            state = next_state\n",
    "            trip_length += 1\n",
    "        lengths.append(trip_length)\n",
    "    avg_len=sum(lengths)/10\n",
    "    return avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.12 12.62 12.38 12.07 12.  ]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "discount_factor = [0.5,0.6,0.7,0.8,0.9]\n",
    "exploration = 0.1\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(discount_factor)):\n",
    "        q_learning(learning_rate,discount_factor[i],exploration,epochs)\n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.56 12.26 12.19 12.21 11.94]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.1,0.2,0.3,0.4,0.5]\n",
    "discount_factor = 0.9\n",
    "exploration = 0.1\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(learning_rate)):\n",
    "        q_learning(learning_rate[i],discount_factor,exploration,epochs)\n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.67 12.2  11.9  12.59]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "discount_factor = 0.5\n",
    "exploration = [0.1,0.2,0.3,0.4]\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(exploration)):\n",
    "        q_learning(learning_rate,discount_factor,exploration[i],epochs)\n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.790000000000001\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.4\n",
    "discount_factor = 0.5\n",
    "exploration = 0.3\n",
    "epochs = 1000\n",
    "difdis=[]\n",
    "for j in range(1,10):\n",
    "        q_learning(learning_rate,discount_factor,exploration,epochs)\n",
    "        difdis.append(average_trip_length())\n",
    "\n",
    "print(sum(difdis)/10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}